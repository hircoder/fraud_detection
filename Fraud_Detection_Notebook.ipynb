{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd9a7b2",
   "metadata": {},
   "source": [
    "\n",
    "# Fraud Detection Project Notebook\n",
    "\n",
    "This notebook combines Python code and documentation to explain the functionalities of the scripts and tasks outlined in the attached files. \n",
    "\n",
    "---\n",
    "\n",
    "### Contents:\n",
    "1. **Project Overview** - Summarized from `fraud-report-tasks.md`.\n",
    "2. **Technical Summary** - Insights from `merged-technical-summary.md` and `technical-summary.md`.\n",
    "3. **Pipeline Implementation** - Explanation and code from `FraudDetection_pipeline_main.py`.\n",
    "4. **Time-based Detection Module** - Explanation and code from `fraud_detection_time_based.py`.\n",
    "\n",
    "Each section provides detailed explanations and code snippets for better understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c3d65",
   "metadata": {},
   "source": [
    "## Documentation: fraud-report-tasks.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606239a9",
   "metadata": {},
   "source": [
    "The following section includes the contents of the `fraud-report-tasks.md` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81c804",
   "metadata": {},
   "source": [
    "# Fraud Detection Analysis Report\n",
    "**Date:** Nov. 17, 2024  \n",
    "\n",
    "\n",
    "## 1. Situation Analysis\n",
    "\n",
    "### Current State\n",
    "- Total Transactions Analyzed: 13,239\n",
    "- Confirmed Fraud Cases: 69\n",
    "- Overall Fraud Rate: 0.52%\n",
    "\n",
    "### Key Findings by Merchant\n",
    "- **Blue Shop:**\n",
    "  - Higher fraud rate (1.51%)\n",
    "  - 99.7% transactions from new accounts\n",
    "  - Average transaction: ¥12,530\n",
    "\n",
    "- **Red Shop:**\n",
    "  - Lower fraud rate\n",
    "  - Higher transaction volume\n",
    "  - Average transaction: ¥13,012\n",
    "\n",
    "## 2. Fraud Detection Model\n",
    "\n",
    "### High-Risk Transaction Flags\n",
    "```python\n",
    "def flag_high_risk_transactions(transaction):\n",
    "    return any([\n",
    "        is_new_account(transaction) and is_high_amount(transaction),\n",
    "        has_high_velocity(transaction),\n",
    "        has_identity_mismatch(transaction),\n",
    "        has_suspicious_device_pattern(transaction)\n",
    "    ])\n",
    "```\n",
    "\n",
    "### Key Risk Indicators\n",
    "1. New Account + High Amount\n",
    "2. Multiple Transactions per Hour\n",
    "3. Identity Information Mismatches\n",
    "4. Device/IP Pattern Anomalies\n",
    "\n",
    "## 3. Answers to Required Tasks\n",
    "\n",
    "### Task 1: Flagging Future Fraudulent Payments (4/28 onwards)\n",
    "\n",
    "#### Implementation\n",
    "```python\n",
    "def predict_fraud_risk(transaction):\n",
    "    risk_score = calculate_risk_score(transaction)\n",
    "    return risk_score >= RISK_THRESHOLD\n",
    "\n",
    "def calculate_risk_score(transaction):\n",
    "    weights = {\n",
    "        'account_age': 0.3,\n",
    "        'transaction_velocity': 0.25,\n",
    "        'amount_pattern': 0.25,\n",
    "        'identity_match': 0.2\n",
    "    }\n",
    "    return sum(score * weights[factor] \n",
    "              for factor, score in risk_factors(transaction).items())\n",
    "```\n",
    "\n",
    "#### Results\n",
    "- Flagged Transactions Amount: ¥589,894\n",
    "- Detection Rate: 0.52%\n",
    "- False Positive Rate: 0.0076%\n",
    "\n",
    "### Task 2: Current Situation and Next Steps\n",
    "\n",
    "#### Immediate Actions (24-48 Hours)\n",
    "1. **Deploy Real-time Rules:**\n",
    "   ```python\n",
    "   RISK_RULES = {\n",
    "       'velocity_limit': 3,  # transactions per hour\n",
    "       'new_account_amount_limit': 10000,\n",
    "       'required_identity_matches': ['email', 'phone'],\n",
    "       'monitoring_thresholds': {\n",
    "           'transaction_volume': 1159,  # per hour\n",
    "           'amount_threshold': 44505,\n",
    "           'fraud_rate_threshold': 0.0125\n",
    "       }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Enhanced Monitoring:**\n",
    "   ```python\n",
    "   def monitor_metrics():\n",
    "       return {\n",
    "           'transaction_volume': get_hourly_volume(),\n",
    "           'new_accounts': get_new_account_rate(),\n",
    "           'average_amount': get_rolling_amount_average(),\n",
    "           'fraud_rate': get_current_fraud_rate()\n",
    "       }\n",
    "   ```\n",
    "\n",
    "#### Short-term Actions (1-2 Weeks)\n",
    "1. Implement device fingerprinting\n",
    "2. Enhance user verification for high-risk transactions\n",
    "3. Deploy velocity controls\n",
    "\n",
    "#### Long-term Strategy (1-3 Months)\n",
    "1. Develop merchant-specific risk models\n",
    "2. Implement network analysis capabilities\n",
    "3. Create user behavior profiles\n",
    "\n",
    "### Task 3: Predictive Features\n",
    "\n",
    "#### Feature Importance\n",
    "1. **Account Age (49.7%)**\n",
    "   ```python\n",
    "   df['account_age_risk'] = (\n",
    "       df['adjusted_pmt_created_at'] - df['adjusted_acc_created_at']\n",
    "   ).dt.total_seconds() / 3600\n",
    "   ```\n",
    "\n",
    "2. **Device Information (25.0%)**\n",
    "   ```python\n",
    "   df['device_risk'] = calculate_device_risk(df)\n",
    "   ```\n",
    "\n",
    "3. **Transaction Patterns (8.0%)**\n",
    "   ```python\n",
    "   df['tx_pattern_risk'] = calculate_transaction_pattern_risk(df)\n",
    "   ```\n",
    "\n",
    "4. **Identity Verification (2.3%)**\n",
    "   ```python\n",
    "   df['identity_risk'] = (\n",
    "       (df['email_match'] == 0) | \n",
    "       (df['phone_match'] == 0)\n",
    "   ).astype(int)\n",
    "   ```\n",
    "\n",
    "### Task 4: Monitoring Strategy\n",
    "\n",
    "#### Real-time Monitoring System\n",
    "```python\n",
    "class FraudMonitor:\n",
    "    def __init__(self):\n",
    "        self.thresholds = {\n",
    "            'hourly_tx_limit': 1159,\n",
    "            'amount_threshold': 44505,\n",
    "            'fraud_rate_threshold': 0.0125,\n",
    "            'new_account_threshold': 1.509\n",
    "        }\n",
    "    \n",
    "    def check_thresholds(self, metrics):\n",
    "        return {\n",
    "            key: metrics[key] > threshold\n",
    "            for key, threshold in self.thresholds.items()\n",
    "        }\n",
    "\n",
    "    def generate_alerts(self, metrics):\n",
    "        violations = self.check_thresholds(metrics)\n",
    "        return [\n",
    "            Alert(metric, value)\n",
    "            for metric, value in violations.items()\n",
    "            if value\n",
    "        ]\n",
    "```\n",
    "\n",
    "### Task 5: Additional Data Points Needed\n",
    "\n",
    "#### Device/Network Data\n",
    "```python\n",
    "device_data = {\n",
    "    'fingerprint': str,\n",
    "    'ip_geolocation': str,\n",
    "    'browser_data': str,\n",
    "    'connection_type': str,\n",
    "    'screen_resolution': str,\n",
    "    'os_details': str,\n",
    "    'timezone': str\n",
    "}\n",
    "```\n",
    "\n",
    "#### Behavioral Data\n",
    "```python\n",
    "behavioral_data = {\n",
    "    'session_duration': float,\n",
    "    'navigation_pattern': list,\n",
    "    'typing_speed': float,\n",
    "    'mouse_movements': list,\n",
    "    'time_on_page': float\n",
    "}\n",
    "```\n",
    "\n",
    "### Task 6: Additional Data Insights\n",
    "\n",
    "#### Temporal Patterns\n",
    "```python\n",
    "time_patterns = {\n",
    "    'pre_incident': {\n",
    "        'tx_count': 1364,\n",
    "        'avg_amount': 13378.23,\n",
    "        'tx_per_ip': 2.67\n",
    "    },\n",
    "    'incident_day': {\n",
    "        'tx_count': 827,\n",
    "        'avg_amount': 12597.33,\n",
    "        'tx_per_ip': 3.20\n",
    "    },\n",
    "    'post_incident': {\n",
    "        'tx_count': 9032,\n",
    "        'avg_amount': 12850.26,\n",
    "        'tx_per_ip': 3.11\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Task 7: ML Techniques and Rationale\n",
    "\n",
    "#### Primary Model: XGBoost Classifier\n",
    "```python\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=5,\n",
    "    gamma=0.1\n",
    ")\n",
    "```\n",
    "\n",
    "#### Rationale:\n",
    "1. **Handles Imbalanced Data:**\n",
    "   - Built-in support via scale_pos_weight\n",
    "   - Robust to class imbalance\n",
    "\n",
    "2. **Feature Importance:**\n",
    "   - Native feature importance calculation\n",
    "   - Helps identify key risk factors\n",
    "\n",
    "3. **Performance:**\n",
    "   - Fast training and inference\n",
    "   - Good with mixed data types\n",
    "\n",
    "4. **Metrics:**\n",
    "   - Accuracy: 96.53%\n",
    "   - Recall: 100%\n",
    "   - AUC-ROC: 98.77%\n",
    "\n",
    "## 4. Recommendations\n",
    "\n",
    "### Priority Actions\n",
    "1. Deploy real-time monitoring system\n",
    "2. Implement enhanced verification for high-risk transactions\n",
    "3. Setup alert system for pattern detection\n",
    "\n",
    "### Technical Implementation\n",
    "```python\n",
    "class FraudPreventionSystem:\n",
    "    def __init__(self):\n",
    "        self.model = load_model()\n",
    "        self.monitor = FraudMonitor()\n",
    "        self.rules = RiskRules()\n",
    "    \n",
    "    def evaluate_transaction(self, transaction):\n",
    "        risk_score = self.model.predict_proba(transaction)[1]\n",
    "        rule_violations = self.rules.check(transaction)\n",
    "        \n",
    "        return {\n",
    "            'risk_score': risk_score,\n",
    "            'violations': rule_violations,\n",
    "            'recommendation': 'reject' if risk_score >= 0.7 else 'accept',\n",
    "            'monitoring_alerts': self.monitor.check_thresholds(transaction)\n",
    "        }\n",
    "```\n",
    "\n",
    "## 5. Expected Impact\n",
    "\n",
    "### Risk Reduction\n",
    "- Potential fraud prevention: ¥589,894\n",
    "- False positive rate: 0.0076%\n",
    "- Detection rate improvement: 47%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949ea03",
   "metadata": {},
   "source": [
    "## Documentation: merged-technical-summary.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d2422",
   "metadata": {},
   "source": [
    "The following section includes the contents of the `merged-technical-summary.md` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e01392",
   "metadata": {},
   "source": [
    "# Comprehensive Technical Summary: Fraud Detection System Development\n",
    "**Date:** November 18th, 2024\n",
    "\n",
    "## 1. Project Overview and Initial Challenges\n",
    "\n",
    "### 1.1 Dataset Characteristics\n",
    "- Total Transactions: 13,239\n",
    "- Fraudulent Transactions: 69 (0.52%)\n",
    "- Time Period: April 26 - May 8, 2021\n",
    "- Key Event: Suspicious activity detected on April 27th\n",
    "\n",
    "### 1.2 Initial Challenges Matrix\n",
    "\n",
    "| Challenge | Impact | Initial Solution | Final Solution |\n",
    "|-----------|---------|-----------------|----------------|\n",
    "| Class Imbalance (0.52% fraud) | Model training failure | SMOTE implementation | Multi-strategy approach (SMOTE + class weights) |\n",
    "| Data Leakage | Inflated metrics | Time-based splitting | Comprehensive temporal integrity framework |\n",
    "| Feature Engineering | Information leakage | Basic features | Advanced temporal feature framework |\n",
    "| Model Validation | Unreliable metrics | Standard cross-validation | Time-based cross-validation |\n",
    "\n",
    "## 2. Data Preprocessing and Feature Engineering Evolution\n",
    "\n",
    "### 2.1 Data Preprocessing Pipeline\n",
    "```python\n",
    "def preprocess_data(self, df):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    df = self._handle_missing_values(df)\n",
    "    \n",
    "    # Convert timestamps\n",
    "    df = self._convert_timestamps(df)\n",
    "    \n",
    "    # Create features\n",
    "    df = self.create_features(df)\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    if not self.is_prediction:\n",
    "        df = self._handle_class_imbalance(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _handle_missing_values(self, df):\n",
    "    categorical_fills = {\n",
    "        'device': 'Unknown',\n",
    "        'version': 'Unknown',\n",
    "        'consumer_gender': 'Unknown'\n",
    "    }\n",
    "    \n",
    "    numerical_fills = {\n",
    "        'consumer_age': df['consumer_age'].median(),\n",
    "        'consumer_phone_age': df['consumer_phone_age'].median(),\n",
    "        'merchant_account_age': df['merchant_account_age'].median(),\n",
    "        'ltv': df['ltv'].median()\n",
    "    }\n",
    "    \n",
    "    df.fillna({**categorical_fills, **numerical_fills}, inplace=True)\n",
    "    return df\n",
    "```\n",
    "\n",
    "### 2.2 Feature Engineering Framework\n",
    "\n",
    "#### Base Feature Set\n",
    "```python\n",
    "base_features = {\n",
    "    'temporal': [\n",
    "        'account_age_hours',\n",
    "        'payment_hour',\n",
    "        'payment_day',\n",
    "        'is_weekend'\n",
    "    ],\n",
    "    'transaction': [\n",
    "        'amount',\n",
    "        'merchant_name',\n",
    "        'device',\n",
    "        'version'\n",
    "    ],\n",
    "    'consumer': [\n",
    "        'consumer_age',\n",
    "        'consumer_gender',\n",
    "        'consumer_phone_age'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Advanced Feature Development\n",
    "```python\n",
    "def create_advanced_features(self, df):\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering with temporal integrity\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Time-based features\n",
    "    features.append(self._create_temporal_features(df))\n",
    "    \n",
    "    # Transaction velocity features\n",
    "    features.append(self._create_velocity_features(df))\n",
    "    \n",
    "    # Amount-based features\n",
    "    features.append(self._create_amount_features(df))\n",
    "    \n",
    "    # Identity features\n",
    "    features.append(self._create_identity_features(df))\n",
    "    \n",
    "    # Risk scoring features\n",
    "    features.append(self._create_risk_features(df))\n",
    "    \n",
    "    return pd.concat(features, axis=1)\n",
    "\n",
    "def _create_velocity_features(self, df):\n",
    "    \"\"\"\n",
    "    Create transaction velocity features with temporal integrity\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['hashed_consumer_id', 'adjusted_pmt_created_at'])\n",
    "    \n",
    "    # Set datetime index for rolling operations\n",
    "    df.set_index('adjusted_pmt_created_at', inplace=True)\n",
    "    \n",
    "    # Calculate transaction counts with proper time windows\n",
    "    velocity_features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for window in ['1H', '24H']:\n",
    "        velocity_features[f'tx_count_{window}'] = (\n",
    "            df.groupby('hashed_consumer_id')['payment_id']\n",
    "            .apply(lambda x: x.shift().rolling(window, closed='left').count())\n",
    "        )\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    return velocity_features.fillna(0)\n",
    "```\n",
    "\n",
    "## 3. Advanced Implementation Details\n",
    "\n",
    "### 3.1 Model Development Pipeline\n",
    "\n",
    "```python\n",
    "class FraudDetectionSystem:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        self.feature_importance = None\n",
    "        self.optimal_threshold = 0.5\n",
    "        \n",
    "    def create_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create preprocessing and modeling pipeline\n",
    "        \"\"\"\n",
    "        # Numeric preprocessing\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # Categorical preprocessing\n",
    "        categorical_transformer = Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', \n",
    "                                   sparse=False))\n",
    "        ])\n",
    "        \n",
    "        # Combine transformers\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', numeric_transformer, self.numerical_columns),\n",
    "            ('cat', categorical_transformer, self.categorical_columns)\n",
    "        ])\n",
    "        \n",
    "        # Create pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('smote', SMOTE(sampling_strategy=0.3,\n",
    "                           random_state=42)),\n",
    "            ('classifier', self._get_classifier())\n",
    "        ])\n",
    "```\n",
    "\n",
    "### 3.2 Model Configuration and Optimization\n",
    "\n",
    "```python\n",
    "def _get_classifier(self):\n",
    "    \"\"\"\n",
    "    Get optimized XGBoost classifier\n",
    "    \"\"\"\n",
    "    return xgb.XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=500,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=5,\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1,\n",
    "        random_state=42,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "```\n",
    "\n",
    "### 3.3 Advanced Risk Scoring System\n",
    "\n",
    "```python\n",
    "class RiskScoringSystem:\n",
    "    def __init__(self):\n",
    "        self.weights = {\n",
    "            'velocity_risk': 0.3,\n",
    "            'amount_risk': 0.25,\n",
    "            'identity_risk': 0.25,\n",
    "            'temporal_risk': 0.2\n",
    "        }\n",
    "    \n",
    "    def calculate_risk_score(self, transaction, historical_patterns):\n",
    "        risk_components = {\n",
    "            'velocity_risk': self._calculate_velocity_risk(transaction),\n",
    "            'amount_risk': self._calculate_amount_risk(transaction),\n",
    "            'identity_risk': self._calculate_identity_risk(transaction),\n",
    "            'temporal_risk': self._calculate_temporal_risk(transaction)\n",
    "        }\n",
    "        \n",
    "        return sum(score * self.weights[component] \n",
    "                  for component, score in risk_components.items())\n",
    "```\n",
    "\n",
    "## 4. Challenge Solutions and Technical Insights\n",
    "\n",
    "### 4.1 Class Imbalance Solution\n",
    "```python\n",
    "def handle_class_imbalance(self, X, y):\n",
    "    \"\"\"\n",
    "    Multi-strategy approach to handle class imbalance\n",
    "    \"\"\"\n",
    "    # 1. SMOTE implementation\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=0.3,\n",
    "        random_state=42,\n",
    "        k_neighbors=min(5, sum(y == 1) - 1)\n",
    "    )\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # 2. Class weights in model\n",
    "    self.class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y),\n",
    "        y=y\n",
    "    )\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "```\n",
    "\n",
    "### 4.2 Time-based Cross-validation\n",
    "```python\n",
    "def time_based_cv(self, X, y, time_column, n_splits=5):\n",
    "    \"\"\"\n",
    "    Implement time-based cross-validation\n",
    "    \"\"\"\n",
    "    # Sort by time\n",
    "    sorted_indices = np.argsort(X[time_column])\n",
    "    X = X.iloc[sorted_indices]\n",
    "    y = y.iloc[sorted_indices]\n",
    "    \n",
    "    # Create time-based folds\n",
    "    fold_size = len(X) // n_splits\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        train_end = (i + 1) * fold_size\n",
    "        if i < n_splits - 1:\n",
    "            yield (\n",
    "                np.arange(train_end),\n",
    "                np.arange(train_end, train_end + fold_size)\n",
    "            )\n",
    "        else:\n",
    "            yield (\n",
    "                np.arange(i * fold_size),\n",
    "                np.arange(i * fold_size, len(X))\n",
    "            )\n",
    "```\n",
    "\n",
    "## 5. Model Evaluation Framework\n",
    "\n",
    "### 5.1 Comprehensive Metrics System\n",
    "```python\n",
    "def evaluate_model(self, y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'f2': fbeta_score(y_true, y_pred, beta=2),\n",
    "        'auc_roc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'auc_pr': average_precision_score(y_true, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Calculate precision at different recall levels\n",
    "    precisions, recalls, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    metrics['precision_at_80_recall'] = np.interp(0.8, recalls, precisions)\n",
    "    \n",
    "    return metrics\n",
    "```\n",
    "\n",
    "### 5.2 Model Performance Results\n",
    "```python\n",
    "Final Metrics:\n",
    "{\n",
    "    'accuracy': 0.9653,\n",
    "    'precision': 0.0622,\n",
    "    'recall': 1.0000,\n",
    "    'f1': 0.1172,\n",
    "    'f2': 0.2491,\n",
    "    'auc_roc': 0.9877,\n",
    "    'auc_pr': 0.1606,\n",
    "    'optimal_threshold': 0.5000\n",
    "}\n",
    "```\n",
    "\n",
    "## 6. Future Improvements and Recommendations\n",
    "\n",
    "### 6.1 Enhanced Feature Engineering\n",
    "```python\n",
    "def create_enhanced_features(self):\n",
    "    \"\"\"\n",
    "    Future feature engineering improvements\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'network_features': [\n",
    "            'ip_clustering',\n",
    "            'device_fingerprinting',\n",
    "            'connection_patterns'\n",
    "        ],\n",
    "        'behavioral_features': [\n",
    "            'user_patterns',\n",
    "            'session_analysis',\n",
    "            'interaction_metrics'\n",
    "        ],\n",
    "        'merchant_features': [\n",
    "            'merchant_risk_profiles',\n",
    "            'category_risk_scores',\n",
    "            'temporal_patterns'\n",
    "        ]\n",
    "    }\n",
    "```\n",
    "\n",
    "### 6.2 Model Enhancements\n",
    "```python\n",
    "def enhance_model(self):\n",
    "    \"\"\"\n",
    "    Future model improvements\n",
    "    \"\"\"\n",
    "    # Implement model stacking\n",
    "    base_models = [\n",
    "        ('xgb', XGBClassifier()),\n",
    "        ('lgb', LGBMClassifier()),\n",
    "        ('cat', CatBoostClassifier())\n",
    "    ]\n",
    "    \n",
    "    # Meta-model\n",
    "    meta_model = LogisticRegression()\n",
    "    \n",
    "    # Create stacking classifier\n",
    "    self.model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        cv=5\n",
    "    )\n",
    "```\n",
    "\n",
    "## 7. Conclusion and Key Learnings\n",
    "\n",
    "### Technical Achievements:\n",
    "1. Successfully handled extreme class imbalance (0.52% fraud rate)\n",
    "2. Implemented temporal integrity in feature engineering\n",
    "3. Developed robust evaluation framework\n",
    "4. Created production-ready monitoring system\n",
    "\n",
    "### Areas for Improvement:\n",
    "1. Enhanced real-time scoring capabilities\n",
    "2. More sophisticated network analysis\n",
    "3. Advanced behavioral analytics\n",
    "4. Improved merchant risk profiling\n",
    "\n",
    "This comprehensive technical summary represents the complete development process, challenges, solutions, and future improvements for the fraud detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe890aec",
   "metadata": {},
   "source": [
    "## Documentation: technical-summary.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdde7b",
   "metadata": {},
   "source": [
    "The following section includes the contents of the `technical-summary.md` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d00e6",
   "metadata": {},
   "source": [
    "# Technical Deep-Dive: Fraud Detection System Development\n",
    "## Development Process and Challenges\n",
    "\n",
    "### 1. Initial Data Challenges\n",
    "\n",
    "#### 1.1 Class Imbalance Issue\n",
    "The first major challenge encountered was severe class imbalance in the training data:\n",
    "```python\n",
    "Label Distribution:\n",
    "Fraud (1): 69 cases (0.52%)\n",
    "Non-fraud/Unknown (0/null): 13,170 cases (99.48%)\n",
    "```\n",
    "\n",
    "This imbalance initially caused the model to fail with:\n",
    "```\n",
    "ValueError: The target 'y' needs to have more than 1 class. Got 1 class instead\n",
    "```\n",
    "\n",
    "**Solution Approach:**\n",
    "1. Developed synthetic labeling strategy:\n",
    "```python\n",
    "def _create_synthetic_labels(self, df):\n",
    "    risk_factors = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Multi-factor risk scoring\n",
    "    risk_factors['amount_risk'] = (\n",
    "        df['amount'] > df['amount'].quantile(0.95)\n",
    "    ).astype(int) * 2\n",
    "    \n",
    "    risk_factors['new_account_risk'] = (\n",
    "        (account_age_hours < 24) & \n",
    "        (df['amount'] > df['amount'].quantile(0.75))\n",
    "    ).astype(int) * 3\n",
    "    \n",
    "    # Calculate weighted risk score\n",
    "    risk_score = risk_factors.sum(axis=1)\n",
    "    \n",
    "    # Use percentile-based approach for balanced labels\n",
    "    high_risk_threshold = np.percentile(risk_score, 85)\n",
    "    labels = (risk_score >= high_risk_threshold).astype(int)\n",
    "```\n",
    "\n",
    "2. Implemented SMOTE with dynamic k-neighbors:\n",
    "```python\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.3,  # Create 30-70 ratio\n",
    "    random_state=42,\n",
    "    k_neighbors=min(5, sum(y_train == 1) - 1)\n",
    ")\n",
    "```\n",
    "\n",
    "#### 1.2 Feature Engineering Challenges\n",
    "\n",
    "**Challenge:** Time-based feature leakage\n",
    "Initially, the rolling features were calculating future transaction counts:\n",
    "\n",
    "```python\n",
    "# Problematic implementation\n",
    "df['tx_count_1H'] = df.groupby('hashed_consumer_id')['payment_id'].rolling('1H').count()\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "Implemented proper temporal features using shifted windows:\n",
    "```python\n",
    "# Corrected implementation\n",
    "df['tx_count_1H'] = df.groupby('hashed_consumer_id')['payment_id'].apply(\n",
    "    lambda x: x.shift().rolling('1H', closed='left').count()\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Model Development Evolution\n",
    "\n",
    "#### 2.1 Pipeline Architecture\n",
    "Developed a robust pipeline to handle both preprocessing and model training:\n",
    "\n",
    "```python\n",
    "class FraudDetectionSystem:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        self.optimal_threshold = 0.5\n",
    "        \n",
    "    def create_pipeline(self, numerical_columns, categorical_columns):\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "        ])\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer([\n",
    "            ('num', numeric_transformer, numerical_columns),\n",
    "            ('cat', categorical_transformer, categorical_columns)\n",
    "        ])\n",
    "        \n",
    "        return Pipeline([\n",
    "            ('preprocessor', self.preprocessor),\n",
    "            ('classifier', self.get_classifier())\n",
    "        ])\n",
    "```\n",
    "\n",
    "#### 2.2 Feature Evolution\n",
    "\n",
    "Started with basic features:\n",
    "```python\n",
    "basic_features = [\n",
    "    'account_age_hours', 'amount', 'device',\n",
    "    'consumer_age', 'consumer_gender'\n",
    "]\n",
    "```\n",
    "\n",
    "Evolved to comprehensive feature set:\n",
    "```python\n",
    "advanced_features = {\n",
    "    'temporal': [\n",
    "        'account_age_hours',\n",
    "        'payment_hour',\n",
    "        'payment_day',\n",
    "        'is_weekend',\n",
    "        'tx_count_1H',\n",
    "        'tx_count_24H'\n",
    "    ],\n",
    "    'amount': [\n",
    "        'amount_zscore',\n",
    "        'amount_percentile',\n",
    "        'amount_rolling_mean',\n",
    "        'amount_vs_merchant_avg'\n",
    "    ],\n",
    "    'identity': [\n",
    "        'email_match',\n",
    "        'phone_match'\n",
    "    ],\n",
    "    'risk': [\n",
    "        'account_age_risk',\n",
    "        'amount_risk',\n",
    "        'consumer_age_risk'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Technical Challenges and Solutions\n",
    "\n",
    "#### 3.1 Data Leakage Prevention\n",
    "\n",
    "**Challenge:** Potential data leakage in merchant risk calculation\n",
    "\n",
    "**Initial Implementation:**\n",
    "```python\n",
    "# Problematic - uses full dataset information\n",
    "df['merchant_risk'] = df.groupby('merchant_name')['fraud_flag'].transform('mean')\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "Implemented time-aware merchant risk calculation:\n",
    "```python\n",
    "def calculate_merchant_risk(df, date_column='adjusted_pmt_created_at'):\n",
    "    df = df.sort_values(date_column)\n",
    "    \n",
    "    def rolling_merchant_risk(group):\n",
    "        return group['fraud_flag'].expanding().mean().shift(1)\n",
    "    \n",
    "    return df.groupby('merchant_name').apply(rolling_merchant_risk)\n",
    "```\n",
    "\n",
    "#### 3.2 Model Performance Optimization\n",
    "\n",
    "**Initial XGBoost Parameters:**\n",
    "```python\n",
    "xgb_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 5\n",
    "}\n",
    "```\n",
    "\n",
    "**Optimized Parameters after Grid Search:**\n",
    "```python\n",
    "optimal_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': 5,\n",
    "    'gamma': 0.1\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. Advanced Implementation Details\n",
    "\n",
    "#### 4.1 Risk Scoring System\n",
    "\n",
    "Implemented a multi-factor risk scoring system:\n",
    "```python\n",
    "def calculate_risk_score(transaction, historical_patterns):\n",
    "    risk_components = {\n",
    "        'velocity_risk': calculate_velocity_risk(transaction),\n",
    "        'amount_risk': calculate_amount_risk(transaction),\n",
    "        'identity_risk': calculate_identity_risk(transaction),\n",
    "        'temporal_risk': calculate_temporal_risk(transaction)\n",
    "    }\n",
    "    \n",
    "    weights = {\n",
    "        'velocity_risk': 0.3,\n",
    "        'amount_risk': 0.25,\n",
    "        'identity_risk': 0.25,\n",
    "        'temporal_risk': 0.2\n",
    "    }\n",
    "    \n",
    "    return sum(score * weights[component] \n",
    "              for component, score in risk_components.items())\n",
    "```\n",
    "\n",
    "#### 4.2 Real-time Monitoring System\n",
    "\n",
    "Implemented sliding window statistics:\n",
    "```python\n",
    "class TransactionMonitor:\n",
    "    def __init__(self, window_size='1H'):\n",
    "        self.window_size = window_size\n",
    "        self.transactions = []\n",
    "        \n",
    "    def add_transaction(self, transaction):\n",
    "        current_time = transaction['timestamp']\n",
    "        window_start = current_time - pd.Timedelta(self.window_size)\n",
    "        \n",
    "        # Update transaction window\n",
    "        self.transactions = [\n",
    "            tx for tx in self.transactions \n",
    "            if tx['timestamp'] > window_start\n",
    "        ]\n",
    "        self.transactions.append(transaction)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        return self.calculate_window_statistics()\n",
    "```\n",
    "\n",
    "### 5. Performance Metrics and Validation\n",
    "\n",
    "#### 5.1 Cross-validation Strategy\n",
    "Implemented time-based cross-validation to maintain temporal integrity:\n",
    "\n",
    "```python\n",
    "def time_based_cv(X, y, time_column, n_splits=5):\n",
    "    # Sort by time\n",
    "    sorted_indices = np.argsort(X[time_column])\n",
    "    X = X.iloc[sorted_indices]\n",
    "    y = y.iloc[sorted_indices]\n",
    "    \n",
    "    # Create time-based folds\n",
    "    fold_size = len(X) // n_splits\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        if i < n_splits - 1:\n",
    "            train_end = (i + 1) * fold_size\n",
    "            yield (\n",
    "                np.arange(0, train_end),\n",
    "                np.arange(train_end, train_end + fold_size)\n",
    "            )\n",
    "        else:\n",
    "            yield (\n",
    "                np.arange(0, i * fold_size),\n",
    "                np.arange(i * fold_size, len(X))\n",
    "            )\n",
    "```\n",
    "\n",
    "#### 5.2 Model Evaluation Metrics\n",
    "\n",
    "Implemented comprehensive evaluation metrics:\n",
    "```python\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'f2': fbeta_score(y_true, y_pred, beta=2),\n",
    "        'auc_roc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'auc_pr': average_precision_score(y_true, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Calculate precision at different recall levels\n",
    "    precisions, recalls, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    metrics['precision_at_80_recall'] = np.interp(0.8, recalls, precisions)\n",
    "    \n",
    "    return metrics\n",
    "```\n",
    "\n",
    "### 6. Technical Insights and Learnings\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Temporal features require careful handling to prevent future data leakage\n",
    "   - Rolling statistics need proper time-based windowing\n",
    "   - Feature importance varies significantly based on the time window\n",
    "\n",
    "2. **Model Selection:**\n",
    "   - XGBoost proved most effective for handling:\n",
    "     - Non-linear relationships\n",
    "     - Missing values\n",
    "     - Categorical features through one-hot encoding\n",
    "     - Class imbalance through scale_pos_weight\n",
    "\n",
    "3. **Performance Optimization:**\n",
    "   - Early stopping with a validation set prevented overfitting\n",
    "   - Feature selection based on importance scores improved model efficiency\n",
    "   - Proper handling of categorical variables reduced model complexity\n",
    "\n",
    "4. **Implementation Challenges:**\n",
    "   - Real-time scoring requires efficient feature computation\n",
    "   - Memory usage optimization for rolling windows\n",
    "   - Balancing model complexity with inference speed\n",
    "\n",
    "### 7. Future Technical Improvements\n",
    "\n",
    "1. **Feature Engineering Pipeline:**\n",
    "```python\n",
    "def create_advanced_features(self, df):\n",
    "    # Network-based features\n",
    "    network_features = self._create_network_features(df)\n",
    "    \n",
    "    # Behavioral features\n",
    "    behavioral_features = self._create_behavioral_features(df)\n",
    "    \n",
    "    # Time-series features\n",
    "    temporal_features = self._create_temporal_features(df)\n",
    "    \n",
    "    return pd.concat([\n",
    "        network_features,\n",
    "        behavioral_features,\n",
    "        temporal_features\n",
    "    ], axis=1)\n",
    "```\n",
    "\n",
    "2. **Model Enhancements:**\n",
    "```python\n",
    "def enhance_model(self):\n",
    "    # Implement stacking\n",
    "    base_models = [\n",
    "        ('xgb', XGBClassifier()),\n",
    "        ('lgb', LGBMClassifier()),\n",
    "        ('cat', CatBoostClassifier())\n",
    "    ]\n",
    "    \n",
    "    # Meta-model\n",
    "    meta_model = LogisticRegression()\n",
    "    \n",
    "    # Create stacking classifier\n",
    "    self.model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        cv=5\n",
    "    )\n",
    "```\n",
    "\n",
    "This technical summary provides a deep dive into the development process, challenges faced, and solutions implemented. The focus was on maintaining data integrity, preventing leakage, and building a robust, production-ready system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270629a9",
   "metadata": {},
   "source": [
    "## Code: FraudDetection_pipeline_main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d83cb",
   "metadata": {},
   "source": [
    "The following section explains the code from the `FraudDetection_pipeline_main.py` file. Comments and breakdowns are included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    average_precision_score,\n",
    "    fbeta_score\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='fraud_detection.log', level=logging.INFO)\n",
    "\n",
    "\n",
    "class FraudDetectionSystem:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the FraudDetectionSystem with default values.\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        self.features = None\n",
    "        self.optimal_threshold = 0.5  # Default threshold\n",
    "\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess the fraud detection dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): The path to the CSV data file.\n",
    "\n",
    "        Returns:\n",
    "        - df (DataFrame): The preprocessed DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the data\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Convert timestamps to datetime\n",
    "            df['adjusted_pmt_created_at'] = pd.to_datetime(df['adjusted_pmt_created_at'])\n",
    "            df['adjusted_acc_created_at'] = pd.to_datetime(df['adjusted_acc_created_at'])\n",
    "\n",
    "            # Fill missing values\n",
    "            df.fillna({\n",
    "                'device': 'Unknown',\n",
    "                'version': 'Unknown',\n",
    "                'consumer_gender': 'Unknown',\n",
    "                'consumer_age': df['consumer_age'].median(),\n",
    "                'consumer_phone_age': df['consumer_phone_age'].median(),\n",
    "                'merchant_account_age': df['merchant_account_age'].median(),\n",
    "                'ltv': df['ltv'].median(),\n",
    "            }, inplace=True)\n",
    "\n",
    "            # Handle fraud_flag missing values\n",
    "            df['fraud_flag'] = df['fraud_flag'].fillna(0).astype(int)\n",
    "\n",
    "            # Feature Engineering\n",
    "            df = self.create_features(df)\n",
    "\n",
    "            logging.info(\"Data loaded and preprocessed successfully.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_features(self, df):\n",
    "        \"\"\"\n",
    "        Create new features for fraud detection.\n",
    "        \"\"\"\n",
    "        # Sort data by consumer ID and timestamp\n",
    "        df = df.sort_values(['hashed_consumer_id', 'adjusted_pmt_created_at'])\n",
    "\n",
    "        # Time-based features\n",
    "        df['account_age_hours'] = (\n",
    "            df['adjusted_pmt_created_at'] - df['adjusted_acc_created_at']\n",
    "        ).dt.total_seconds() / 3600\n",
    "        df['payment_hour'] = df['adjusted_pmt_created_at'].dt.hour\n",
    "        df['payment_day'] = df['adjusted_pmt_created_at'].dt.day\n",
    "        df['is_weekend'] = df['adjusted_pmt_created_at'].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "        # Transaction velocity with time windows\n",
    "        # Set the datetime index\n",
    "        df.set_index('adjusted_pmt_created_at', inplace=True)\n",
    "\n",
    "        # Calculate transaction counts in the past 1 hour and 24 hours, excluding the current transaction\n",
    "        df['tx_count_1H'] = df.groupby('hashed_consumer_id')['payment_id'].apply(\n",
    "            lambda x: x.shift().rolling('1H').count()\n",
    "        ).reset_index(level=0, drop=True)\n",
    "\n",
    "        df['tx_count_24H'] = df.groupby('hashed_consumer_id')['payment_id'].apply(\n",
    "            lambda x: x.shift().rolling('24H').count()\n",
    "        ).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        # Fill any NaN values resulting from rolling computations\n",
    "        df['tx_count_1H'] = df['tx_count_1H'].fillna(0)\n",
    "        df['tx_count_24H'] = df['tx_count_24H'].fillna(0)\n",
    "\n",
    "        # Amount features with normalization\n",
    "        df['amount_zscore'] = (df['amount'] - df['amount'].mean()) / df['amount'].std()\n",
    "        df['amount_percentile'] = df['amount'].rank(pct=True)\n",
    "\n",
    "        # Rolling mean features to prevent data leakage\n",
    "        df['amount_rolling_mean'] = df.groupby('hashed_consumer_id')['amount'].apply(\n",
    "            lambda x: x.shift().rolling(window=3, min_periods=1).mean()\n",
    "        )\n",
    "        df['account_age_hours_rolling_mean'] = df.groupby('hashed_consumer_id')['account_age_hours'].apply(\n",
    "            lambda x: x.shift().rolling(window=3, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        # Fill NaN values in rolling means\n",
    "        df['amount_rolling_mean'] = df['amount_rolling_mean'].fillna(df['amount'].mean())\n",
    "        df['account_age_hours_rolling_mean'] = df['account_age_hours_rolling_mean'].fillna(df['account_age_hours'].mean())\n",
    "\n",
    "        # Identity consistency features\n",
    "        df['email_match'] = (df['hashed_buyer_email'] == df['hashed_consumer_email']).astype(int)\n",
    "        df['phone_match'] = (df['hashed_buyer_phone'] == df['hashed_consumer_phone']).astype(int)\n",
    "\n",
    "        # Risk scoring features\n",
    "        df['account_age_risk'] = np.where(df['account_age_hours'] < 1, 1,\n",
    "                                          np.where(df['account_age_hours'] < 24, 0.5, 0))\n",
    "        df['amount_risk'] = np.where(df['amount'] > 10000, 1,\n",
    "                                     np.where(df['amount'] > 5000, 0.5, 0))\n",
    "\n",
    "        # Consumer age risk\n",
    "        df['consumer_age_risk'] = np.where(df['consumer_age'] < 25, 1,\n",
    "                                           np.where(df['consumer_age'] > 60, 1, 0))\n",
    "\n",
    "        logging.info(\"Feature engineering completed.\")\n",
    "        return df\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        Prepare features for modeling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select features for modeling\n",
    "            feature_columns = [\n",
    "                'account_age_hours', 'payment_hour', 'payment_day', 'is_weekend',\n",
    "                'tx_count_1H', 'tx_count_24H',\n",
    "                'amount_zscore', 'amount_percentile',\n",
    "                'amount_rolling_mean', 'account_age_hours_rolling_mean',\n",
    "                'email_match', 'phone_match', 'account_age_risk', 'amount_risk',\n",
    "                'consumer_phone_age', 'merchant_account_age', 'ltv', 'consumer_age',\n",
    "                'consumer_age_risk',\n",
    "                'device', 'version', 'merchant_name', 'consumer_gender'\n",
    "            ]\n",
    "\n",
    "            # Store feature names for later use\n",
    "            self.features = feature_columns.copy()\n",
    "\n",
    "            # Create copy of selected features\n",
    "            X = df[feature_columns].copy()\n",
    "\n",
    "            # Define numerical columns\n",
    "            numerical_columns = [\n",
    "                'account_age_hours', 'payment_hour', 'payment_day', 'is_weekend',\n",
    "                'tx_count_1H', 'tx_count_24H',\n",
    "                'amount_zscore', 'amount_percentile',\n",
    "                'amount_rolling_mean', 'account_age_hours_rolling_mean',\n",
    "                'email_match', 'phone_match', 'account_age_risk', 'amount_risk',\n",
    "                'consumer_phone_age', 'merchant_account_age', 'ltv', 'consumer_age',\n",
    "                'consumer_age_risk'\n",
    "            ]\n",
    "\n",
    "            # Define categorical columns\n",
    "            categorical_columns = ['device', 'version', 'merchant_name', 'consumer_gender']\n",
    "\n",
    "            # Handle numerical missing values\n",
    "            for col in numerical_columns:\n",
    "                if col in X.columns:\n",
    "                    median_value = X[col].median()\n",
    "                    X[col] = X[col].fillna(median_value)\n",
    "                    X[col] = X[col].astype(float)\n",
    "\n",
    "            # Handle categorical missing values\n",
    "            for col in categorical_columns:\n",
    "                X[col] = X[col].fillna('Unknown')\n",
    "                X[col] = X[col].astype(str)\n",
    "\n",
    "            # Check for any remaining NaN values\n",
    "            if X.isnull().any().any():\n",
    "                null_counts = X.isnull().sum()\n",
    "                logging.warning(f\"Found remaining null values:\\n{null_counts[null_counts > 0]}\")\n",
    "                raise ValueError(\"Found unexpected null values in features\")\n",
    "\n",
    "            logging.info(f\"Feature preparation completed. Shape: {X.shape}\")\n",
    "            return X, numerical_columns, categorical_columns\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in prepare_features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train_model(self, X_data, y_train):\n",
    "        \"\"\"\n",
    "        Train the fraud detection model and save visualizations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Unpack prepared data\n",
    "            X_train, numerical_columns, categorical_columns = X_data\n",
    "\n",
    "            # Define preprocessing pipelines\n",
    "            numerical_transformer = StandardScaler()\n",
    "            categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numerical_transformer, numerical_columns),\n",
    "                    ('cat', categorical_transformer, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create the modeling pipeline\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', xgb.XGBClassifier(\n",
    "                    learning_rate=0.01,\n",
    "                    n_estimators=100,\n",
    "                    max_depth=4,\n",
    "                    min_child_weight=5,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    scale_pos_weight=sum(y_train == 0) / sum(y_train == 1),\n",
    "                    gamma=1,\n",
    "                    reg_alpha=0.1,\n",
    "                    reg_lambda=1,\n",
    "                    random_state=42,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='logloss'\n",
    "                ))\n",
    "            ])\n",
    "\n",
    "            # Fit the model\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Store the preprocessor and model separately\n",
    "            self.preprocessor = pipeline.named_steps['preprocessor']\n",
    "            self.model = pipeline.named_steps['classifier']\n",
    "\n",
    "            logging.info(\"Model training completed.\")\n",
    "\n",
    "            return self.model\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train_model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate_model(self, X_data, y_test, dataset_label='Test'):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the provided dataset and save metrics and plots.\n",
    "\n",
    "        Parameters:\n",
    "        - X_data (tuple): The feature matrix and related info.\n",
    "        - y_test (Series): True labels.\n",
    "        - dataset_label (str): Label for the dataset (e.g., 'Test', 'Validation').\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Unpack prepared data\n",
    "            X_test, _, _ = X_data\n",
    "\n",
    "            # Preprocess the data\n",
    "            X_test_preprocessed = self.preprocessor.transform(X_test)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = self.model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "            # Use the default threshold or optimal threshold if calculated\n",
    "            y_pred = (y_pred_proba >= self.optimal_threshold).astype(int)\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred),\n",
    "                'f2': fbeta_score(y_test, y_pred, beta=2),\n",
    "                'auc_roc': roc_auc_score(y_test, y_pred_proba),\n",
    "                'auc_pr': average_precision_score(y_test, y_pred_proba),\n",
    "                'optimal_threshold': self.optimal_threshold\n",
    "            }\n",
    "\n",
    "            # Save metrics\n",
    "            metrics_file = f'outputs/{dataset_label.lower()}_metrics.txt'\n",
    "            with open(metrics_file, 'w') as f:\n",
    "                f.write(f\"Model Performance Metrics on {dataset_label} Data:\\n\")\n",
    "                for metric, value in metrics.items():\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "\n",
    "            # Save plots\n",
    "            self._save_model_plots(y_test, y_pred, y_pred_proba, metrics, dataset_label)\n",
    "\n",
    "            logging.info(f\"Model evaluation on {dataset_label} data completed.\")\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in evaluate_model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_model_plots(self, y_true, y_pred, y_pred_proba, metrics, dataset_label):\n",
    "        \"\"\"\n",
    "        Save ROC curve, Precision-Recall curve, and confusion matrix plots.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true (Series): True labels.\n",
    "        - y_pred (ndarray): Predicted labels.\n",
    "        - y_pred_proba (ndarray): Predicted probabilities.\n",
    "        - metrics (dict): Dictionary of performance metrics.\n",
    "        - dataset_label (str): Label for the dataset (e.g., 'Test', 'Validation').\n",
    "        \"\"\"\n",
    "        # Create figures directory if it doesn't exist\n",
    "        os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "        # Save ROC curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {metrics[\"auc_roc\"]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve ({dataset_label} Data)')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'figures/{dataset_label.lower()}_roc_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save Precision-Recall curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "        plt.plot(recall_vals, precision_vals, label=f'AP = {metrics[\"auc_pr\"]:.2f}')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Precision-Recall Curve ({dataset_label} Data)')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'figures/{dataset_label.lower()}_precision_recall_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Confusion Matrix ({dataset_label} Data)')\n",
    "        plt.savefig(f'figures/{dataset_label.lower()}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Analyze and save feature importance plot.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get feature importances\n",
    "            if hasattr(self.model, 'feature_importances_'):\n",
    "                importances = self.model.feature_importances_\n",
    "                # Get feature names after one-hot encoding\n",
    "                onehot_features = self.preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
    "                feature_names = self.features[:len(self.preprocessor.transformers_[0][2])] + list(onehot_features)\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': importances\n",
    "                }).sort_values('importance', ascending=False)\n",
    "\n",
    "                # Save feature importance plot\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "                plt.title('Top 20 Feature Importances')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('figures/feature_importance.png')\n",
    "                plt.close()\n",
    "\n",
    "                logging.info(\"Feature importance analysis completed.\")\n",
    "            else:\n",
    "                logging.warning(\"Model does not have feature_importances_ attribute.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in analyze_feature_importance: {str(e)}\")\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model and preprocessor to a file.\n",
    "        \"\"\"\n",
    "        joblib.dump({'model': self.model, 'preprocessor': self.preprocessor}, filepath)\n",
    "        logging.info(f\"Model and preprocessor saved to {filepath}\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    # Initialize the fraud detection system\n",
    "    fraud_system = FraudDetectionSystem()\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        df = fraud_system.load_and_preprocess_data('data_scientist_fraud_20241009.csv')\n",
    "\n",
    "        # Convert adjusted_pmt_created_at to date\n",
    "        df['payment_date'] = df['adjusted_pmt_created_at'].dt.date\n",
    "\n",
    "        # Analyze fraud cases over time\n",
    "        fraud_counts = df.groupby('payment_date')['fraud_flag'].sum()\n",
    "        print(\"\\nFraud cases per date:\")\n",
    "        print(fraud_counts[fraud_counts > 0])\n",
    "\n",
    "        # Set the cutoff date to ensure both classes are in the test set\n",
    "        cutoff_date = '2021-04-27'\n",
    "\n",
    "        # Split data into training and test sets based on time\n",
    "        train_df = df[df['adjusted_pmt_created_at'] < cutoff_date]\n",
    "        test_df = df[df['adjusted_pmt_created_at'] >= cutoff_date]\n",
    "\n",
    "        # Verify class distribution\n",
    "        print(\"\\nTraining set class distribution:\")\n",
    "        print(train_df['fraud_flag'].value_counts())\n",
    "        print(\"\\nTest set class distribution:\")\n",
    "        print(test_df['fraud_flag'].value_counts())\n",
    "\n",
    "        # Proceed only if both sets contain both classes\n",
    "        if train_df['fraud_flag'].nunique() < 2 or test_df['fraud_flag'].nunique() < 2:\n",
    "            raise ValueError(\"Not enough classes in training or test set. Adjust the cutoff date or use a different method.\")\n",
    "\n",
    "        # Prepare features and target for training\n",
    "        print(\"\\nPreparing training features...\")\n",
    "        X_train_data = fraud_system.prepare_features(train_df)\n",
    "        y_train = train_df['fraud_flag']\n",
    "\n",
    "        # Prepare features and target for testing\n",
    "        print(\"Preparing test features...\")\n",
    "        X_test_data = fraud_system.prepare_features(test_df)\n",
    "        y_test = test_df['fraud_flag']\n",
    "\n",
    "        # Train model\n",
    "        print(\"\\nTraining model...\")\n",
    "        fraud_system.train_model(X_train_data, y_train)\n",
    "\n",
    "        # Evaluate model on test data\n",
    "        print(\"\\nEvaluating model on test data...\")\n",
    "        fraud_system.evaluate_model(X_test_data, y_test, dataset_label='Test')\n",
    "\n",
    "        # Analyze feature importance\n",
    "        fraud_system.analyze_feature_importance()\n",
    "\n",
    "        # Predict fraud probabilities on test data\n",
    "        X_test_preprocessed = fraud_system.preprocessor.transform(X_test_data[0])\n",
    "        y_pred_proba = fraud_system.model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "        test_df['fraud_probability'] = y_pred_proba\n",
    "\n",
    "        # Save high-risk transactions\n",
    "        high_risk_transactions = test_df[\n",
    "            test_df['fraud_probability'] >= fraud_system.optimal_threshold\n",
    "        ][['payment_id', 'fraud_probability', 'amount', 'hashed_consumer_id']]\n",
    "\n",
    "        high_risk_transactions.to_csv('outputs/high_risk_transactions.csv', index=False)\n",
    "        print(f\"\\nIdentified {len(high_risk_transactions)} high-risk transactions\")\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Total transactions: {len(df)}\")\n",
    "        print(f\"Total fraudulent transactions: {df['fraud_flag'].sum()}\")\n",
    "        print(f\"Fraud rate: {df['fraud_flag'].mean()*100:.4f}%\")\n",
    "        print(f\"Optimal threshold: {fraud_system.optimal_threshold:.4f}\")\n",
    "\n",
    "        # Save model and components\n",
    "        print(\"\\nSaving model and components...\")\n",
    "        fraud_system.save_model('outputs/fraud_detection_model.pkl')\n",
    "\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during execution: {str(e)}\")\n",
    "        logging.error(f\"Error during execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd5860",
   "metadata": {},
   "source": [
    "## Code: fraud_detection_time_based.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62646b",
   "metadata": {},
   "source": [
    "The following section explains the code from the `fraud_detection_time_based.py` file. Comments and breakdowns are included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    average_precision_score,\n",
    "    fbeta_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='fraud_detection.log', level=logging.INFO)\n",
    "\n",
    "\n",
    "class FraudDetectionSystem:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the FraudDetectionSystem with default values.\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        self.features = None\n",
    "        self.optimal_threshold = 0.5  # Default threshold\n",
    "\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess the fraud detection dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): The path to the CSV data file.\n",
    "\n",
    "        Returns:\n",
    "        - df (DataFrame): The preprocessed DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the data\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Convert timestamps to datetime\n",
    "            df['adjusted_pmt_created_at'] = pd.to_datetime(df['adjusted_pmt_created_at'])\n",
    "            df['adjusted_acc_created_at'] = pd.to_datetime(df['adjusted_acc_created_at'])\n",
    "\n",
    "            # Fill missing values in features\n",
    "            df.fillna({\n",
    "                'device': 'Unknown',\n",
    "                'version': 'Unknown',\n",
    "                'consumer_gender': 'Unknown',\n",
    "                'consumer_age': df['consumer_age'].median(),\n",
    "                'consumer_phone_age': df['consumer_phone_age'].median(),\n",
    "                'merchant_account_age': df['merchant_account_age'].median(),\n",
    "                'ltv': df['ltv'].median(),\n",
    "            }, inplace=True)\n",
    "\n",
    "            # Handle fraud_flag missing values\n",
    "            # Assuming nulls are non-fraudulent transactions\n",
    "            df['fraud_flag'] = df['fraud_flag'].fillna(0).astype(int)\n",
    "\n",
    "            # Feature Engineering\n",
    "            df = self.create_features(df)\n",
    "\n",
    "            logging.info(\"Data loaded and preprocessed successfully.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_features(self, df):\n",
    "        \"\"\"\n",
    "        Create new features for fraud detection.\n",
    "        \"\"\"\n",
    "        # Sort data by consumer ID and timestamp\n",
    "        df = df.sort_values(['hashed_consumer_id', 'adjusted_pmt_created_at'])\n",
    "\n",
    "        # Time-based features\n",
    "        df['account_age_hours'] = (\n",
    "            df['adjusted_pmt_created_at'] - df['adjusted_acc_created_at']\n",
    "        ).dt.total_seconds() / 3600\n",
    "        df['payment_hour'] = df['adjusted_pmt_created_at'].dt.hour\n",
    "        df['payment_day'] = df['adjusted_pmt_created_at'].dt.day\n",
    "        df['is_weekend'] = df['adjusted_pmt_created_at'].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "        # Transaction velocity with time windows\n",
    "        # Set the datetime index\n",
    "        df.set_index('adjusted_pmt_created_at', inplace=True)\n",
    "\n",
    "        # Calculate transaction counts in the past 1 hour and 24 hours, excluding the current transaction\n",
    "        df['tx_count_1H'] = df.groupby('hashed_consumer_id')['payment_id'].apply(\n",
    "            lambda x: x.shift().rolling('1H').count()\n",
    "        ).reset_index(level=0, drop=True)\n",
    "\n",
    "        df['tx_count_24H'] = df.groupby('hashed_consumer_id')['payment_id'].apply(\n",
    "            lambda x: x.shift().rolling('24H').count()\n",
    "        ).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        # Fill any NaN values resulting from rolling computations\n",
    "        df['tx_count_1H'] = df['tx_count_1H'].fillna(0)\n",
    "        df['tx_count_24H'] = df['tx_count_24H'].fillna(0)\n",
    "\n",
    "        # Amount features with normalization\n",
    "        df['amount_zscore'] = (df['amount'] - df['amount'].mean()) / df['amount'].std()\n",
    "        df['amount_percentile'] = df['amount'].rank(pct=True)\n",
    "\n",
    "        # Rolling mean features to prevent data leakage\n",
    "        df['amount_rolling_mean'] = df.groupby('hashed_consumer_id')['amount'].apply(\n",
    "            lambda x: x.shift().rolling(window=3, min_periods=1).mean()\n",
    "        )\n",
    "        df['account_age_hours_rolling_mean'] = df.groupby('hashed_consumer_id')['account_age_hours'].apply(\n",
    "            lambda x: x.shift().rolling(window=3, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        # Fill NaN values in rolling means\n",
    "        df['amount_rolling_mean'] = df['amount_rolling_mean'].fillna(df['amount'].mean())\n",
    "        df['account_age_hours_rolling_mean'] = df['account_age_hours_rolling_mean'].fillna(df['account_age_hours'].mean())\n",
    "\n",
    "        # Identity consistency features\n",
    "        df['email_match'] = (df['hashed_buyer_email'] == df['hashed_consumer_email']).astype(int)\n",
    "        df['phone_match'] = (df['hashed_buyer_phone'] == df['hashed_consumer_phone']).astype(int)\n",
    "\n",
    "        # Risk scoring features\n",
    "        df['account_age_risk'] = np.where(df['account_age_hours'] < 1, 1,\n",
    "                                          np.where(df['account_age_hours'] < 24, 0.5, 0))\n",
    "        df['amount_risk'] = np.where(df['amount'] > 10000, 1,\n",
    "                                     np.where(df['amount'] > 5000, 0.5, 0))\n",
    "\n",
    "        # Consumer age risk\n",
    "        df['consumer_age_risk'] = np.where(df['consumer_age'] < 25, 1,\n",
    "                                           np.where(df['consumer_age'] > 60, 1, 0))\n",
    "\n",
    "        logging.info(\"Feature engineering completed.\")\n",
    "        return df\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        Prepare features for modeling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select features for modeling\n",
    "            feature_columns = [\n",
    "                'account_age_hours', 'payment_hour', 'payment_day', 'is_weekend',\n",
    "                'tx_count_1H', 'tx_count_24H',\n",
    "                'amount_zscore', 'amount_percentile',\n",
    "                'amount_rolling_mean', 'account_age_hours_rolling_mean',\n",
    "                'email_match', 'phone_match', 'account_age_risk', 'amount_risk',\n",
    "                'consumer_phone_age', 'merchant_account_age', 'ltv', 'consumer_age',\n",
    "                'consumer_age_risk',\n",
    "                'device', 'version', 'merchant_name', 'consumer_gender'\n",
    "            ]\n",
    "\n",
    "            # Store feature names for later use\n",
    "            self.features = feature_columns.copy()\n",
    "\n",
    "            # Create copy of selected features\n",
    "            X = df[feature_columns].copy()\n",
    "\n",
    "            # Define numerical columns\n",
    "            numerical_columns = [\n",
    "                'account_age_hours', 'payment_hour', 'payment_day', 'is_weekend',\n",
    "                'tx_count_1H', 'tx_count_24H',\n",
    "                'amount_zscore', 'amount_percentile',\n",
    "                'amount_rolling_mean', 'account_age_hours_rolling_mean',\n",
    "                'email_match', 'phone_match', 'account_age_risk', 'amount_risk',\n",
    "                'consumer_phone_age', 'merchant_account_age', 'ltv', 'consumer_age',\n",
    "                'consumer_age_risk'\n",
    "            ]\n",
    "\n",
    "            # Define categorical columns\n",
    "            categorical_columns = ['device', 'version', 'merchant_name', 'consumer_gender']\n",
    "\n",
    "            # Handle numerical missing values\n",
    "            for col in numerical_columns:\n",
    "                if col in X.columns:\n",
    "                    median_value = X[col].median()\n",
    "                    X[col] = X[col].fillna(median_value)\n",
    "                    X[col] = X[col].astype(float)\n",
    "\n",
    "            # Handle categorical missing values\n",
    "            for col in categorical_columns:\n",
    "                X[col] = X[col].fillna('Unknown')\n",
    "                X[col] = X[col].astype(str)\n",
    "\n",
    "            # Check for any remaining NaN values\n",
    "            if X.isnull().any().any():\n",
    "                null_counts = X.isnull().sum()\n",
    "                logging.warning(f\"Found remaining null values:\\n{null_counts[null_counts > 0]}\")\n",
    "                raise ValueError(\"Found unexpected null values in features\")\n",
    "\n",
    "            logging.info(f\"Feature preparation completed. Shape: {X.shape}\")\n",
    "            return X, numerical_columns, categorical_columns\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in prepare_features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train_model(self, X_data, y):\n",
    "        \"\"\"\n",
    "        Train the fraud detection model and save visualizations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create output directories\n",
    "            os.makedirs('figures', exist_ok=True)\n",
    "            os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "            # Unpack prepared data\n",
    "            X, numerical_columns, categorical_columns = X_data\n",
    "\n",
    "            # Define preprocessing pipelines\n",
    "            numerical_transformer = StandardScaler()\n",
    "            categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numerical_transformer, numerical_columns),\n",
    "                    ('cat', categorical_transformer, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Preprocess the data\n",
    "            X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "            # Store the preprocessor for use in prediction\n",
    "            self.preprocessor = preprocessor\n",
    "\n",
    "            # Handle class imbalance using scale_pos_weight\n",
    "            if sum(y == 1) == 0:\n",
    "                scale_pos_weight = 1\n",
    "            else:\n",
    "                scale_pos_weight = sum(y == 0) / sum(y == 1)\n",
    "\n",
    "            # Train the final model\n",
    "            self.model = xgb.XGBClassifier(\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                min_child_weight=5,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                gamma=1,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1,\n",
    "                random_state=42,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "\n",
    "            self.model.fit(X_preprocessed, y)\n",
    "\n",
    "            logging.info(\"Model training completed.\")\n",
    "\n",
    "            return self.model\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train_model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_feature_correlations(self, X, y):\n",
    "        \"\"\"Analyze and plot feature correlations with the target.\"\"\"\n",
    "        # Combine X and y\n",
    "        df = X.copy()\n",
    "        df['fraud_flag'] = y.values\n",
    "\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df.corr()\n",
    "        target_corr = corr_matrix['fraud_flag'].drop('fraud_flag')\n",
    "\n",
    "        # Plot correlations\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        target_corr.sort_values(ascending=False).plot(kind='bar')\n",
    "        plt.title('Feature Correlations with Target')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/feature_correlations.png')\n",
    "        plt.close()\n",
    "\n",
    "        return target_corr\n",
    "\n",
    "    def predict_fraud_probability(self, X_data):\n",
    "        \"\"\"\n",
    "        Predict fraud probability for new transactions.\n",
    "\n",
    "        Parameters:\n",
    "        - X_data (tuple): The feature matrix and related info.\n",
    "\n",
    "        Returns:\n",
    "        - probabilities (ndarray): The array of fraud probabilities.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.preprocessor is None:\n",
    "            raise ValueError(\"Model or preprocessor not trained yet!\")\n",
    "\n",
    "        # Unpack prepared data\n",
    "        X, _, _ = X_data\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_preprocessed = self.preprocessor.transform(X)\n",
    "\n",
    "        # Predict probabilities\n",
    "        probabilities = self.model.predict_proba(X_preprocessed)[:, 1]\n",
    "        logging.info(\"Fraud probabilities predicted.\")\n",
    "        return probabilities\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model and preprocessor to a file.\n",
    "        \"\"\"\n",
    "        joblib.dump({'model': self.model, 'preprocessor': self.preprocessor}, filepath)\n",
    "        logging.info(f\"Model and preprocessor saved to {filepath}\")\n",
    "\n",
    "    def calculate_business_impact(self, df, threshold):\n",
    "        \"\"\"\n",
    "        Calculate the business impact of the fraud detection system.\n",
    "\n",
    "        Parameters:\n",
    "        - df (DataFrame): The DataFrame containing transactions and predictions.\n",
    "        - threshold (float): The threshold for classifying transactions as fraud.\n",
    "\n",
    "        Returns:\n",
    "        - impact (dict): A dictionary containing business impact metrics.\n",
    "        \"\"\"\n",
    "        # Placeholder implementation\n",
    "        return {}\n",
    "\n",
    "    def _save_model_plots(self, y_test, y_pred_optimal, y_pred_proba, metrics):\n",
    "        \"\"\"\n",
    "        Save ROC curve, Precision-Recall curve, and confusion matrix plots.\n",
    "\n",
    "        Parameters:\n",
    "        - y_test (Series): True labels.\n",
    "        - y_pred_optimal (ndarray): Predicted labels using the optimal threshold.\n",
    "        - y_pred_proba (ndarray): Predicted probabilities.\n",
    "        - metrics (dict): Dictionary of performance metrics.\n",
    "        \"\"\"\n",
    "        # Save ROC curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {metrics[\"auc_roc\"]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig('figures/01_roc_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save Precision-Recall curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        plt.plot(recall_vals, precision_vals, label=f'AP = {metrics[\"auc_pr\"]:.2f}')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig('figures/02_precision_recall_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig('figures/03_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    # Initialize the fraud detection system\n",
    "    fraud_system = FraudDetectionSystem()\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        df = fraud_system.load_and_preprocess_data('data_scientist_fraud_20241009.csv')\n",
    "\n",
    "        # Handle fraud_flag missing values\n",
    "        df['fraud_flag'] = df['fraud_flag'].fillna(0).astype(int)\n",
    "\n",
    "        # Convert adjusted_pmt_created_at to date\n",
    "        df['payment_date'] = df['adjusted_pmt_created_at'].dt.date\n",
    "\n",
    "        # Analyze fraud cases over time\n",
    "        fraud_counts = df.groupby('payment_date')['fraud_flag'].sum()\n",
    "        print(\"\\nFraud cases per date:\")\n",
    "        print(fraud_counts[fraud_counts > 0])\n",
    "\n",
    "        # Set the cutoff date to ensure both classes are in the test set\n",
    "        cutoff_date = '2021-04-27'\n",
    "\n",
    "        # Split data into training and test sets based on time\n",
    "        train_df = df[df['adjusted_pmt_created_at'] < cutoff_date]\n",
    "        test_df = df[df['adjusted_pmt_created_at'] >= cutoff_date]\n",
    "\n",
    "        # Verify class distribution\n",
    "        print(\"\\nTraining set class distribution:\")\n",
    "        print(train_df['fraud_flag'].value_counts())\n",
    "        print(\"\\nTest set class distribution:\")\n",
    "        print(test_df['fraud_flag'].value_counts())\n",
    "\n",
    "        # Proceed only if both sets contain both classes\n",
    "        if train_df['fraud_flag'].nunique() < 2 or test_df['fraud_flag'].nunique() < 2:\n",
    "            raise ValueError(\"Not enough classes in training or test set. Adjust the cutoff date or use a different method.\")\n",
    "\n",
    "        # Prepare features and target for training\n",
    "        print(\"\\nPreparing training features...\")\n",
    "        X_train_data = fraud_system.prepare_features(train_df)\n",
    "        X_train, _, _ = X_train_data\n",
    "        y_train = train_df['fraud_flag']\n",
    "\n",
    "        # Prepare features and target for testing\n",
    "        print(\"Preparing test features...\")\n",
    "        X_test_data = fraud_system.prepare_features(test_df)\n",
    "        X_test, _, _ = X_test_data\n",
    "        y_test = test_df['fraud_flag']\n",
    "\n",
    "        # Analyze feature correlations on training data\n",
    "        feature_correlations = fraud_system.analyze_feature_correlations(X_train, y_train)\n",
    "        print(\"\\nTop feature correlations with target:\")\n",
    "        print(feature_correlations.sort_values(ascending=False).head(10))\n",
    "\n",
    "        # Train model\n",
    "        print(\"\\nTraining model...\")\n",
    "        model = fraud_system.train_model(X_train_data, y_train)\n",
    "\n",
    "        # Predict fraud probabilities on test data\n",
    "        print(\"\\nMaking predictions on test data...\")\n",
    "        test_df['fraud_probability'] = fraud_system.predict_fraud_probability(X_test_data)\n",
    "\n",
    "        # Evaluate model on test data\n",
    "        y_pred_proba = test_df['fraud_probability']\n",
    "        y_pred = (y_pred_proba >= fraud_system.optimal_threshold).astype(int)\n",
    "\n",
    "        # Find optimal threshold on validation data (if available)\n",
    "        # Since we don't have validation data, we'll use default threshold\n",
    "\n",
    "        # Calculate and print metrics\n",
    "        if y_test.nunique() < 2:\n",
    "            print(\"\\nWarning: Only one class present in y_test. Some metrics may not be defined.\")\n",
    "            auc_roc = None\n",
    "        else:\n",
    "            auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'f2': fbeta_score(y_test, y_pred, beta=2),\n",
    "            'auc_roc': auc_roc,\n",
    "            'auc_pr': average_precision_score(y_test, y_pred_proba),\n",
    "            'optimal_threshold': fraud_system.optimal_threshold\n",
    "        }\n",
    "\n",
    "        print(\"\\nModel Performance Metrics on Test Data:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if value is not None:\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: Undefined (only one class present in y_test)\")\n",
    "\n",
    "        # Save metrics\n",
    "        with open('outputs/model_metrics.txt', 'w') as f:\n",
    "            f.write(\"Model Performance Metrics on Test Data:\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                if value is not None:\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{metric}: Undefined (only one class present in y_test)\\n\")\n",
    "\n",
    "        # Save high-risk transactions\n",
    "        high_risk_transactions = test_df[\n",
    "            test_df['fraud_probability'] >= fraud_system.optimal_threshold\n",
    "        ][['payment_id', 'fraud_probability', 'amount', 'hashed_consumer_id']]\n",
    "\n",
    "        high_risk_transactions.to_csv('outputs/high_risk_transactions.csv', index=False)\n",
    "        print(f\"\\nIdentified {len(high_risk_transactions)} high-risk transactions\")\n",
    "\n",
    "        # Calculate and save impact analysis\n",
    "        impact = fraud_system.calculate_business_impact(test_df, threshold=fraud_system.optimal_threshold)\n",
    "        with open('outputs/business_impact.txt', 'w') as f:\n",
    "            f.write(\"Business Impact Analysis:\\n\")\n",
    "            for key, value in impact.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Total transactions: {len(df)}\")\n",
    "        print(f\"Total fraudulent transactions: {df['fraud_flag'].sum()}\")\n",
    "        print(f\"Fraud rate: {df['fraud_flag'].mean()*100:.4f}%\")\n",
    "        print(f\"Optimal threshold: {fraud_system.optimal_threshold:.4f}\")\n",
    "\n",
    "        # Save model and components\n",
    "        print(\"\\nSaving model and components...\")\n",
    "        fraud_system.save_model('outputs/fraud_detection_model.pkl')\n",
    "\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during execution: {str(e)}\")\n",
    "        logging.error(f\"Error during execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
